{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chasing the Platypus via MG-RAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial covers downloading all public MG-RAST datasets; parsing those files to search for a specific taxa of interest, in our case Platypus (Ornithorhynchus); and finding the geographycal location of the samples that match those samples, via Google. Note that you need [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/#Download/) installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading all public MG-RAST datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to crawl MG-RAST's website to search for all public datasets, downloading both the data (in HTML format) and the metadata (tab separated) for each study and store them locally. To avoid having hundreds of files in a single directory the script will group them in folders by the first 3 characters of its name. Note that MG-RAST calls each sample a project, here we will call them samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "from urllib2 import urlopen, URLError\n",
    "from os.path import isfile, exists, join\n",
    "from os import makedirs\n",
    "from socket import timeout as TOError\n",
    "\n",
    "mgrast_analysis = 'http://metagenomics.anl.gov/metagenomics.cgi?page=Analysis'\n",
    "mgrast_metagenome = ('http://metagenomics.anl.gov/metagenomics.cgi?page='\n",
    "                     'MetagenomeOverview&metagenome=')\n",
    "mgrast_metadata = ('http://metagenomics.anl.gov/metagenomics.cgi?page='\n",
    "                   'MetagenomeProject&action=download_md&filetype=text&'\n",
    "                   'filename=mgm%s.metadata.txt')\n",
    "\n",
    "page = urlopen(mgrast_analysis)\n",
    "soup = BeautifulSoup(page.read())\n",
    "count = 0\n",
    "all_inputs = soup.findAll('input', {'id': 'list_select_data_0'})[0]\n",
    "print '====> Got Analysis page results'\n",
    "for maingroups in all_inputs['value'].encode('utf-8').split('||'):\n",
    "    for mg in maingroups.split('@'):\n",
    "        sample = mg.split('##')[0].split('~#')[0]\n",
    "        # will group by the first 3 digits of the project name\n",
    "        dirname = sample[:3]\n",
    "\n",
    "        sample_fp = join(dirname, 'sample_' + sample + '.txt')\n",
    "        map_fp = join(dirname, 'map_' + sample + '.txt')\n",
    "        \n",
    "        count = count + 1\n",
    "        if count % 50 == 0:\n",
    "            print \"====> Processed %d samples\" % count\n",
    "        # continue if sample is blank or file already exists\n",
    "        if sample == '' or (isfile(sample_fp) and isfile(map_fp)):\n",
    "            continue\n",
    "\n",
    "        print 'Processing (%d): %s %s' % (count, sample, mg)\n",
    "\n",
    "        if not exists(dirname):\n",
    "            makedirs(dirname)\n",
    "\n",
    "        # visiting the Overview page to download counts\n",
    "        try:\n",
    "            page_sample = urlopen(mgrast_metagenome + sample, timeout=40)\n",
    "            page_mapping = urlopen(mgrast_metadata % sample, timeout=40)\n",
    "        except (TOError, URLError), e:\n",
    "            # Timeout\n",
    "            print e\n",
    "            count = count - 1\n",
    "            continue\n",
    "        \n",
    "        open(sample_fp, 'w').write(page_sample.read())\n",
    "        open(map_fp, 'w').write(page_mapping.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding taxa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all MG-RAST files in our local folder we can search for any taxa of interest. Note that the HMTL filess that we downloaded in the previous step have all data information that MG-RAST provides from a sample, for example: multilevel taxonomic and functional assignments. Thus, in the next step you can search by any \"string\" that fits in those categories. The output of this section is tab separated text file named as the query (lower case) with two columns: sample name and counts of the successful search on that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "from os import listdir, makedirs\n",
    "from os.path import isdir, isfile, join, basename, exists\n",
    "\n",
    "query = 'Ornithorhynchus'\n",
    "query = query.lower()\n",
    "output_fp = open(query + '.txt', 'w')\n",
    "\n",
    "# looping in local dir to find downloaded folders\n",
    "# all mgrast created folders are 3 chars long and start with a 4\n",
    "dirs = [d for d in listdir('./')\n",
    "        if len(d) == 3 and d.startswith('4') and isdir(d)]\n",
    "\n",
    "for d in dirs:\n",
    "    # getting files in current directory\n",
    "    files = [join(d, f) for f in listdir(d) if f.startswith('sample_')]\n",
    "    for sample in files:\n",
    "        sample_id = basename(f)[len('sample_'):-len('.txt')]\n",
    "        mapping = join(d, 'map_%s.txt' % sample_id)\n",
    "\n",
    "        # validating that we have both the sample and the metadata file for\n",
    "        # the given project\n",
    "        if not isfile(sample) or not isfile(mapping):\n",
    "            raise ValueError('Missing files %s, %s' % (sample, mapping))\n",
    "\n",
    "        soup = BeautifulSoup(open(sample, 'U').read())\n",
    "        for s in soup.findAll('script'):\n",
    "            s = str(s).lower()\n",
    "            \n",
    "            if query in s:\n",
    "                target = [t for t in s.split('\\n') if query in t][0]\n",
    "                counts = target.split(',')[1]\n",
    "                counts = counts[:counts.find(']')]\n",
    "                output_fp.write('%s\\t%s\\n' % (sample_id, counts))\n",
    "                \n",
    "output_fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding geographical localtion of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will search for the geographical location of the successful samples from the previous step using the latitute and longitude values from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "import json\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "\n",
    "input_fp = 'ornithorhynchus.txt'\n",
    "\n",
    "# from http://stackoverflow.com/a/20169528/4228285\n",
    "def getplace(lat, lon):\n",
    "    url = (\"http://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "           \"latlng=%s,%s&sensor=false\" % (lat, lon))\n",
    "    v = urlopen(url).read()\n",
    "    j = json.loads(v)\n",
    "\n",
    "    if len(j['results'])==0:\n",
    "        return 'Undefined'\n",
    "\n",
    "    components = j['results'][0]['address_components']\n",
    "    country = town = None\n",
    "    for c in components:\n",
    "        if \"country\" in c['types']:\n",
    "            country = c['long_name']\n",
    "\n",
    "    return str(country)\n",
    "\n",
    "\n",
    "countries = {}\n",
    "for line in open(input_fp, 'r'):\n",
    "    sample, counts = line.strip().split('\\t')\n",
    "    counts = int(counts)\n",
    "    mapping = join(sample[:3], 'map_%s.txt' % sample)\n",
    "    \n",
    "    if not isfile(mapping):\n",
    "        raise ValueError('Check the existance of: %s' % (mapping))\n",
    "    \n",
    "    lat = ''\n",
    "    lon = ''\n",
    "    for line in open(mapping, 'r'):\n",
    "        line = line.split('\\t')\n",
    "        \n",
    "        # sometimes the lat/lon have different comma separated values\n",
    "        # will only use the first one\n",
    "        if line[0] == 'Sample' and line[1] == 'longitude':\n",
    "            lon = float(line[2].strip().split(',')[0].strip())\n",
    "        elif line[0] == 'Sample' and line[1] == 'latitude':\n",
    "            lat = float(line[2].strip().split(',')[0].strip())\n",
    "        \n",
    "        if not lat or not lon:\n",
    "            country = 'Undefined'\n",
    "        else:\n",
    "            country = getplace(lat, lon)\n",
    "        \n",
    "        if country not in countries:\n",
    "            countries[country] = counts\n",
    "        else:\n",
    "            countries[country] = countries[country] + counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once those results are parsed you could use http://jsfiddle.net/a1royfjh/ to plot the resuls by copy/pasting the result of the next block in the bottom-left section. You need to replace the lines that are all the way to the left, which contains the country and counts information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for country, counts in countries.items():\n",
    "    if country == 'Undefined':\n",
    "        print \"// ['%s', %d],\" % (country, counts)\n",
    "    else:\n",
    "        print \"['%s', %d],\" % (country, counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
